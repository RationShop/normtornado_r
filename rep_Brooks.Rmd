Harold Brooks and others used data from the Storm Prediction Center to estimate the probability of occurrence of a tronado day near any location in the mainland United States (Brooks, Harold E., Charles A. Doswell III, and Michael P. Kay. "Climatological estimates of local daily tornado probability for the United States." Weather and Forecasting 18.4 (2003): 626-640.). 

The below code produces numbers consistent, if not identical, to those from Brooks et al. The discrepancy could be due to the differences in Tornado data used (due to regular quality control the latest data used here may not be identical to the one used by SPC).

Read raw data add column names similar to those used by [Elsner et al 2013](http://rpubs.com/jelsner/4205) and consistent with [documentation](http://www.spc.noaa.gov/gis/svrgis/). 

```{r}
library(plyr)
library(ggplot2)

torn <- read.csv("data/1950-2012_torn.csv", 
                 header = FALSE, 
                 sep = ",", 
                 as.is = TRUE)

colnames(torn) <- c("OM", "YEAR", "MONTH", "DAY", "DATE", "TIME", "TIMEZONE", 
                    "STATE", "FIPS", "STATENUMBER", "FSCALE", "INJURIES", 
                    "FATALITIES", "LOSS", "CROPLOSS", "SLAT", "SLON", "ELAT", 
                    "ELON", "LENGTH", "WIDTH", "NS", "SN", "SG", "F1", "F2", 
                    "F3", "F4")
```

Tornadoes spanning multiple counties are listed separately for each county. Thus, a single tornado could appear multiple times. Identify unique tornadoes using YEAR, OM and NS. Check for uniqueness; especially, tornadoes spanning multiple years (i.e, those which begin on 12/31 and end on 1/1); need to check only those with NS > 1.

```{r}
dec31 <- subset(torn, MONTH == 12 & DAY == 31 & NS != 1)
jan01 <- subset(torn, MONTH == 1 & DAY == 1 & NS != 1)
if (nrow(dec31) > 0 & nrow(jan01) > 0) {
  stop("check! unique id assignment may not be accurate!")
}
torn$id <- paste(torn$YEAR, torn$MONTH, torn$OM, torn$NS, sep = "-")
```

Brooks et al used data from 1955-99 and excludd Alaska, Hawaii and Puerto Rico. 

```{r}
torn <- subset(torn, YEAR %in% seq(1955, 1999))
torn <- subset(torn, !(STATE %in% c("AK", "HI", "PR")))

str(torn)

head(torn)
tail(torn)
```

Function to summarize counts of unique number of tornadoes by year and month.

```{r}
count_unique_tornadoes <- function(in_df, monthly_stats = TRUE) {
  
  require(plyr)
  
  if (monthly_stats) {
    # some months dont have data; assign NAs to those months
    mon_totals <- expand.grid(MONTH = seq(1, 12), stringsAsFactors = FALSE)
    
    # number of unique tornadoes per month
    mon_torn <- ddply(.data = in_df, 
                      .variables = .(MONTH),
                      .fun = function(x_df) length(unique(x_df$id)), 
                      .drop = FALSE)
    
    mon_totals <- merge(mon_totals, mon_torn, by = "MONTH", all = TRUE)
    
    # output matrix
    out_mat <- c(nrow(in_df), length(unique(in_df$id)), mon_totals$V1)
    out_mat <- matrix(out_mat, nrow = 1)
    colnames(out_mat) <- c("N_total", "N_unique", month.abb)  
  } else {
    # output matrix
    out_mat <- c(nrow(in_df), length(unique(in_df$id)))
    out_mat <- matrix(out_mat, nrow = 1)
    colnames(out_mat) <- c("N_total", "N_unique")      
  }
  
  return (out_mat)
}
```

### Figure 1
Counts of tornadoes by year, similar to those in Figure 1 of Brooks et al.

```{r}
event_stats <- ddply(.data = torn, 
                     .variables = .(YEAR), 
                     .fun = count_unique_tornadoes,
                     monthly_stats = FALSE)

str(event_stats)

event_stats
```

Graphic similar to Figure 1 of Brooks et al.

```{r}
gfx_line <- ggplot(data = event_stats, aes(x = YEAR, y = N_unique))
gfx_line <- gfx_line + geom_point()
gfx_line <- gfx_line + ylim(c(0, 1600))
gfx_line <- gfx_line + stat_smooth(method = lm, se = FALSE)
gfx_line <- gfx_line + xlab("Year") + ylab("Tornadoes")
print(gfx_line)
```

### Figure 2
Number of torado days per year, similar to those in Figure 2 of Brooks et al.

```{r}
day_stats <- unique(torn$DATE)
day_stats <- do.call("rbind", strsplit(day_stats, "-"))
head(day_stats)

day_stats <- as.data.frame(table(day_stats[, 1]), stringsAsFactors = FALSE)
colnames(day_stats) <- c("YEAR", "Days")

head(day_stats)
```

Graphic similar to Figure 2 of Brooks et al.

```{r}
gfx_line <- ggplot(data = day_stats, aes(x = YEAR, y = Days, group = 1))
gfx_line <- gfx_line + geom_point()
gfx_line <- gfx_line + ylim(c(0, 250))
gfx_line <- gfx_line + stat_smooth(method = lm, se = FALSE)
gfx_line <- gfx_line + xlab("Year") + ylab("Tornado Days")
print(gfx_line)
```


### Figure 3, Temporal Smoothing

Estimate probability of a tornado day for a leap year, consistent with Brooks et al. Estimate the number of a time a day of the year (doy) occurs in the period 1980-1999. Use 2012 as a reference leap year for the inclusion of Feb 29. One could accomplish the below task using "chron" or "lubridate" package; but, for now I am trying to avoid the importing of these packages.

```{r}
torn_8099 <- subset(torn, YEAR >= 1980 & YEAR <= 1999)
torn_8099 <- subset(torn_8099, !(SLAT == 0 | SLON == 0))

doy <- unique(torn_8099$DATE)
doy <- substr(doy, 6, 10)
doy <- paste0("2012-", doy)
ref_Dec31_2011 <- as.numeric(as.Date("2011-12-31"))
doy_freq <- as.numeric(as.Date(doy)) - ref_Dec31_2011
str(doy_freq)
```

Convert doy_freq to probability and check for probs outside of 0-1 interval. Also prob for Feb 29 should be based on number of leap years during the data time period and not the entire time span, consistent with Brooks et al.

```{r}
time_span <- 1999 - 1980 + 1
doy_prob <- as.data.frame(table(doy_freq), stringsAsFactors = FALSE)
colnames(doy_prob) <- c("doy", "freq")
# fix for Feb 29, 5 leap years in 1980-1999
doy_prob$span <- ifelse(doy_prob$doy != 60, time_span, 5)
doy_prob$prob <- doy_prob$freq / doy_prob$span

# add 0s for missing days
miss_days <- data.frame(doy = as.character(seq(1:366)),
                        DATE = seq(as.Date("2012-01-01"), as.Date("2012-12-31"), by = "day"),
                        stringsAsFactors = FALSE)
doy_prob <- merge(doy_prob, miss_days, by = "doy", all = TRUE, sort = FALSE)
doy_prob$prob[is.na(doy_prob$prob)] <- 0
doy_prob$freq[is.na(doy_prob$freq)] <- 0
doy_prob$span[is.na(doy_prob$span)] <- time_span

# discard "2012"
doy_prob$DATE <- substr(doy_prob$DATE, 6, 10)

head(doy_prob)                         
summary(doy_prob)
```

Probability of a tornado day, unsmoothed, similar to Figure 3 of Brooks et al.

```{r}
gfx_line <- ggplot(data = doy_prob, aes(x = as.numeric(doy), y = prob, group = 1))
gfx_line <- gfx_line + geom_point()
gfx_line <- gfx_line + xlab("Day of Year") + ylab("Probability")
print(gfx_line)
```

Temporal smoothing, similar to Equation 1. Test temporal smoothing using sigma values of of 5 and 15 days. Make the data periodic to avoid problems with beginning and ending of data, cosistent with Brooks et al. Split the 366 days into two 183-day long portions. Add bottom 183-day portion to the top of the data and the top 183-day portion to the bottom of the data. The result is a 732-day long data. Apply frequency smoothing to the middle 366 day of this 732-day data.

```{r}
doy_prob <- rbind(doy_prob[184:366, ], doy_prob, doy_prob[1:183, ])

sigma_t <- 5 # temporal smoother
time_wts5 <- exp(-0.5 * (c(-183:182)/ sigma_t)^2) / (sqrt(2 * pi) * sigma_t)
sigma_t <- 15 # temporal smoother
time_wts15 <- exp(-0.5 * (c(-183:182)/ sigma_t)^2) / (sqrt(2 * pi) * sigma_t)

doy_prob$freq_smooth5 <- 0
doy_prob$freq_smooth15 <- 0
for (eachDay in c(184:549)) {
  beg_index <- eachDay - 183
  end_index <- eachDay + 182
  doy_prob$freq_smooth5[eachDay] <- sum(doy_prob$freq[beg_index:end_index] * time_wts5) 
  doy_prob$freq_smooth15[eachDay] <- sum(doy_prob$freq[beg_index:end_index] * time_wts15) 
}

# discard half cycles at the top and bottom, added previously
doy_prob <- doy_prob[c(184:549), ]
# feb 29 span of 5 days does not apply after smoothing
doy_prob$prob_smooth5 <- doy_prob$freq_smooth5 / time_span
doy_prob$prob_smooth15 <- doy_prob$freq_smooth15 / time_span

summary(doy_prob)
```

Probability of a tornado day, similar to Figure 3 of Brooks et al.

```{r}
gfx_line <- ggplot(data = doy_prob, aes(x = as.numeric(doy), y = prob, group = 1))
gfx_line <- gfx_line + geom_point()
gfx_line <- gfx_line + geom_line(aes(y = prob_smooth5, colour = "red"))
gfx_line <- gfx_line + geom_line(aes(y = prob_smooth15, colour = "blue"))
gfx_line <- gfx_line + theme(legend.position = "none")
gfx_line <- gfx_line + xlab("Day of Year") + ylab("Probability")
print(gfx_line)
```

### Figure 4, Spatial Smoothing

Select relevant data for spatial smoothing and subsequent analysis. Add temporal smoothed info to the torn 1980-99 dataset.

```{r}
torn_8099$DATE <- substr(torn_8099$DATE, 6, 10) # discard year
doy_prob <- doy_prob[, c("doy", "freq", "DATE", "freq_smooth15")]
torn_8099 <- merge(torn_8099, doy_prob, by = "DATE", all = TRUE, sort = FALSE)
torn_8099 <- torn_8099[, c("DATE", "SLAT", "SLON", "ELAT", "ELON", "LENGTH", 
                           "id", "doy", "freq", "freq_smooth15")]
# convert doy from char to int
torn_8099$doy <- as.numeric(torn_8099$doy)

str(torn_8099)
```

Create a uniform lat-lon matrix/grid of 0.5 degrees spanning the mainland US. Brooks et al used a uniform grid of 80-km. But for ease of computation, visualization and storage a 0.5 degree grid is used. The results from this approach will be compared with those from Brooks et al, and if needed, a different spatial grid could be used.

```{r}
lat_seq <- seq(24, 50, 2)
lon_seq <- seq(-125, -65, 2)
ll_mat <- matrix(data = 0, nrow = length(lat_seq) * length(lon_seq), ncol = 366, byrow = TRUE)
```

For each day identify the 0.5 degree boxes/grids which experienced tornadoes. For some tornadoes both the beginning and ending lat-lon values are available. Interpolate between the beginning and ending coordinates and identify all the 0.5 degree grids corresponding to these interpolated points.

Function which takes two points (lat and lon for each point) and returns the 0.5 degree grids associated with these two points and also those points lying on a straight line between them. The function returns a vector of strings, of the form "aa _ bb", where "aa" is the index in "lat seq" and "bb" is the index in "lon seq".

```{r}
Fn_Identify_Grids <- function(coord_vec) {
  begLat <- coord_vec[1]
  begLon <- coord_vec[2]
  endLat <- coord_vec[3]
  endLon <- coord_vec[4]
  
  if (!(begLat == 0 | begLon == 0 | sum(any(is.na(coord_vec))) > 0)) {
  
    out_list <- list()
  
    # indices of beginning point in the lat-lon grid
    out_list <- c(out_list, 
                  paste0(findInterval(begLat, lat_seq), "_", findInterval(begLon, lon_seq)))
    
    if (endLat != 0 & endLon != 0) {
      # indices of ending point in the lat-lon grid
      out_list <- c(out_list, 
                    paste0(findInterval(endLat, lat_seq), "_", findInterval(endLon, lon_seq)))
      
      # indices of in-between points
      # interpolate by making cuts at approx 5-km intervals
      if (endLat > begLat) {
        bet_lats <- seq(begLat, endLat, 0.05)
      } else {
        bet_lats <- seq(begLat, endLat, -0.05)
      }
      
      if (endLon > begLon) {
        bet_lons <- seq(begLon, endLon, 0.05) 
      } else {
        bet_lons <- seq(begLon, endLon, -0.05)
      }
      bet_lats <- findInterval(bet_lats, lat_seq)
      bet_lons <- findInterval(bet_lons, lon_seq)
      bet_indices <- paste0(bet_lats, "_", bet_lons)
      out_list <- c(out_list, bet_indices)
    } 
    
    # identify unique indices
    out_list <- unlist(out_list, use.names = FALSE)
    out_list <- unique(out_list)
    
    return (out_list)
  } else {
    return (NA)
  }
}
```

Use the above function to identify the grids with tornado occurrences for each day of the year.

```{r}
for (eachDoy in c(1:366)) {
  # data for each doy
  torn_doy <- subset(torn_8099, doy == eachDoy)
  torn_doy <- as.matrix(torn_doy[, c("SLAT", "SLON", "ELAT", "ELON", "freq_smooth15")])
  
  # grids corresp to all lat-lon pairs for the doy
  doy_grids <- apply(torn_doy, 1, FUN = Fn_Identify_Grids)
  if (sum(any(is.na(doy_grids))) == 0) {
    doy_grids <- unique(unlist(doy_grids, use.names = FALSE))
    
    # convert these indices to locations in the lat-lon matrix
    ll_index <- do.call("rbind", strsplit(doy_grids, "_"))
    ll_index <- as.matrix(ll_index)
    ll_index <- apply(ll_index, c(1, 2), FUN = as.numeric)
    ll_index <- ll_index[, 2] + (ll_index[, 1] - 1) * length(lon_seq)
    
    # populate the parent matrix, when there is an occurence, with the smoothed
    # temporal frequency
    ll_mat[c(ll_index), eachDoy] <- unique(torn_doy[, "freq_smooth15"])
  }
}
```

Function to compute distance between 2 points on a sphere from their longitudes and latitudes

```{r}
# from http://nssdc.gsfc.nasa.gov/planetary/factsheet/earthfact.html
earthRad <- 6371.0 # volumetric mean radius of earth in km

deg2rad  <- pi / 180.0 # factor to convert degrees to radians
rad2deg  <- 1.0 / deg2rad # factor to convert radians to degrees

Fn_Haversine_Distance <- function(lat1, lon1, lat2, lon2) {
  ## inputs in degrees
  x1 <- lon1*deg2rad
  y1 <- lat1*deg2rad
  x2 <- lon2*deg2rad
  y2 <- lat2*deg2rad
  
  term1 <- (sin((y1-y2)/2))^2
  term2 <- cos(y1) * cos(y2) * (sin((x1-x2)/2))^2
  term3 <- term1 + term2
  
  #   #from http://en.wikipedia.org/wiki/Haversine_formula
  #   dist  <- earthRad * 2 * asin(sqrt(term3))
  #from http://www.movable-type.co.uk/scripts/latlong.html
  dist  <- earthRad * 2 * atan2(sqrt(term3), sqrt(1-term3))
  
  return (dist)
}

```

Calculate spatial weights, using Equation 2.

```{r}
ll_df <- expand.grid(lon = lon_seq, lat = lat_seq, KEEP.OUT.ATTRS = TRUE)
dist_mat <- matrix(0, 
                   nrow = length(lat_seq) * length(lon_seq), 
                   ncol = length(lat_seq) * length(lon_seq), 
                   byrow = TRUE)
for (eachRow in 1:length(lat_seq)) {
  for (eachCol in 1:length(lon_seq)) {
    dist_mat[eachCol + (eachRow - 1) * length(lon_seq), ] <- Fn_Haversine_Distance(lat_seq[eachRow], lon_seq[eachCol], ll_df$lat, ll_df$lon)
  }
}

space_mat <- matrix(data = 0, 
                    nrow = length(lat_seq) * length(lon_seq), 
                    ncol = 366, 
                    byrow = TRUE)
sigma_x <- 120 # spatial smoother in km
for (eachRow in 1:length(lat_seq)) {
  for (eachCol in 1:length(lon_seq)) {
    index <- eachCol + (eachRow - 1) * length(lon_seq)
    for (eachDoy in c(1:366)) {
      space_mat[index, eachDoy] <- sum(ll_mat[, eachDoy] * 
                                         exp(-0.5 * (dist_mat[index, ] / sigma_x)^2))
      space_mat[index, eachDoy] <- space_mat[index, eachDoy] / (2 * pi * (sigma_x^2))
    }
  }
}
```


```{r}
ann_avg <- apply(space_mat, 1, FUN = sum)
summary(ann_avg)
```
