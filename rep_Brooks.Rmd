Harold Brooks and others used data from the Storm Prediction Center to estimate the probability of occurrence of a tronado day near any location in the mainland United States (Brooks, Harold E., Charles A. Doswell III, and Michael P. Kay. "Climatological estimates of local daily tornado probability for the United States." Weather and Forecasting 18.4 (2003): 626-640.). 

The below code produces numbers consistent, if not identical, to those from Brooks et al. The discrepancy could be due to the differences in Tornado data used (due to regular quality control the latest data used here may not be identical to the one used by SPC).

Read raw data add column names similar to those used by [Elsner et al 2013](http://rpubs.com/jelsner/4205) and consistent with [documentation](http://www.spc.noaa.gov/gis/svrgis/). 

```{r}
library(plyr)
library(ggplot2)
library(chron)
library(raster)
library(maptools)
library(rgdal)
library(maps)

torn <- read.csv("data/1950-2012_torn.csv", 
                 header = FALSE, 
                 sep = ",", 
                 as.is = TRUE)

colnames(torn) <- c("OM", "YEAR", "MONTH", "DAY", "DATE", "TIME", "TIMEZONE", 
                    "STATE", "FIPS", "STATENUMBER", "FSCALE", "INJURIES", 
                    "FATALITIES", "LOSS", "CROPLOSS", "SLAT", "SLON", "ELAT", 
                    "ELON", "LENGTH", "WIDTH", "NS", "SN", "SG", "F1", "F2", 
                    "F3", "F4")
```

Tornadoes spanning multiple counties are listed separately for each county. Thus, a single tornado could appear multiple times. Identify unique tornadoes using YEAR, OM and NS. Check for uniqueness; especially, tornadoes spanning multiple years (i.e, those which begin on 12/31 and end on 1/1); need to check only those with NS > 1.

```{r}
dec31 <- subset(torn, MONTH == 12 & DAY == 31 & NS != 1)
jan01 <- subset(torn, MONTH == 1 & DAY == 1 & NS != 1)
if (nrow(dec31) > 0 & nrow(jan01) > 0) {
  stop("check! unique id assignment may not be accurate!")
}
torn$id <- paste(torn$YEAR, torn$MONTH, torn$OM, torn$NS, sep = "-")
```

Brooks et al used data from 1955-99 and excluded Alaska, Hawaii and Puerto Rico. 

```{r}
# torn <- subset(torn, YEAR %in% seq(1955, 1999))
torn <- subset(torn, !(STATE %in% c("AK", "HI", "PR")))

str(torn)

head(torn)
tail(torn)
```

Function to summarize counts of unique number of tornadoes by year and month.

```{r}
count_unique_tornadoes <- function(in_df, monthly_stats = TRUE) {
  
  require(plyr)
  
  if (monthly_stats) {
    # some months dont have data; assign NAs to those months
    mon_totals <- expand.grid(MONTH = seq(1, 12), stringsAsFactors = FALSE)
    
    # number of unique tornadoes per month
    mon_torn <- ddply(.data = in_df, 
                      .variables = .(MONTH),
                      .fun = function(x_df) length(unique(x_df$id)), 
                      .drop = FALSE)
    
    mon_totals <- merge(mon_totals, mon_torn, by = "MONTH", all = TRUE)
    
    # output matrix
    out_mat <- c(nrow(in_df), length(unique(in_df$id)), mon_totals$V1)
    out_mat <- matrix(out_mat, nrow = 1)
    colnames(out_mat) <- c("N_total", "N_unique", month.abb)  
  } else {
    # output matrix
    out_mat <- c(nrow(in_df), length(unique(in_df$id)))
    out_mat <- matrix(out_mat, nrow = 1)
    colnames(out_mat) <- c("N_total", "N_unique")      
  }
  
  return (out_mat)
}
```

### Figure 1
Counts of tornadoes by year, similar to those in Figure 1 of Brooks et al.

```{r}
event_stats <- ddply(.data = torn, 
                     .variables = .(YEAR), 
                     .fun = count_unique_tornadoes,
                     monthly_stats = FALSE)

str(event_stats)

event_stats
```

Graphic similar to Figure 1 of Brooks et al.

```{r}
gfx_line <- ggplot(data = event_stats, aes(x = YEAR, y = N_unique))
gfx_line <- gfx_line + geom_point()
gfx_line <- gfx_line + ylim(c(0, 1600))
gfx_line <- gfx_line + stat_smooth(method = lm, se = FALSE)
gfx_line <- gfx_line + xlab("Year") + ylab("Tornadoes")
png("gfx_brooks2003_fig1.png")
plot(gfx_line)
garbage <- dev.off()
```
![figure 1] [brooks1]

### Figure 2
Number of torado days per year, similar to those in Figure 2 of Brooks et al.

```{r}
day_stats <- unique(torn$DATE)
day_stats <- do.call("rbind", strsplit(day_stats, "-"))
head(day_stats)

day_stats <- as.data.frame(table(day_stats[, 1]), stringsAsFactors = FALSE)
colnames(day_stats) <- c("YEAR", "Days")

head(day_stats)
```

Graphic similar to Figure 2 of Brooks et al.

```{r}
gfx_line <- ggplot(data = day_stats, aes(x = YEAR, y = Days, group = 1))
gfx_line <- gfx_line + geom_point()
gfx_line <- gfx_line + ylim(c(0, 250))
gfx_line <- gfx_line + stat_smooth(method = lm, se = FALSE)
gfx_line <- gfx_line + xlab("Year") + ylab("Tornado Days")
png("gfx_brooks2003_fig2.png")
plot(gfx_line)
garbage <- dev.off()
```
![figure 2] [brooks2]

### Figure 3, Temporal Smoothing

Estimate probability of a tornado day for a leap year, consistent with Brooks et al. Estimate the number of a time a day of the year (doy) occurs in the period 1980-1999. Use 2012 as a reference leap year for the inclusion of Feb 29 (does not matter which leap year is chosen).

```{r}
beg_year <- 1980
end_year <- 1999
torn_brooks <- subset(torn, YEAR >= beg_year & YEAR <= end_year)
torn_brooks <- subset(torn_brooks, !(SLAT == 0 | SLON == 0))

doy <- unique(torn_brooks$DATE)
doy <- substr(doy, 6, 10)
doy <- paste0("2012-", doy)
ref_Dec31_2011 <- as.numeric(as.Date("2011-12-31"))
doy_freq <- as.numeric(as.Date(doy)) - ref_Dec31_2011
str(doy_freq)
```

Convert "doy _ freq" to probability and check for probs outside of 0-1 interval. Also prob for Feb 29 should be based on number of leap years during the data time period and not the entire time span, consistent with Brooks et al.

```{r}
time_span <- length(beg_year:end_year)
doy_prob <- as.data.frame(table(doy_freq), stringsAsFactors = FALSE)
colnames(doy_prob) <- c("doy", "freq")
# fix for Feb 29, leap years in 1980-1999
num_leap <- sum(leap.year(beg_year:end_year))
doy_prob$span <- ifelse(doy_prob$doy != 60, time_span, num_leap)
doy_prob$prob <- doy_prob$freq / doy_prob$span

# add 0s for missing days
miss_days <- data.frame(doy = as.character(seq(1:366)),
                        DATE = seq(as.Date("2012-01-01"), as.Date("2012-12-31"), by = "day"),
                        stringsAsFactors = FALSE)
doy_prob <- merge(doy_prob, miss_days, by = "doy", all = TRUE, sort = FALSE)
doy_prob$prob[is.na(doy_prob$prob)] <- 0
doy_prob$freq[is.na(doy_prob$freq)] <- 0
doy_prob$span[is.na(doy_prob$span)] <- time_span

# discard "2012"
doy_prob$DATE <- substr(doy_prob$DATE, 6, 10)

head(doy_prob)                         
summary(doy_prob)
```

Probability of a tornado day, unsmoothed, similar to Figure 3 of Brooks et al.

```{r}
gfx_line <- ggplot(data = doy_prob, aes(x = as.numeric(doy), y = prob, group = 1))
gfx_line <- gfx_line + geom_point()
gfx_line <- gfx_line + xlab("Day of Year") + ylab("Probability")
# print(gfx_line)
```

Temporal smoothing, similar to Equation 1. Test temporal smoothing using sigma values of of 5 and 15 days. Make the data periodic to avoid problems with beginning and ending of data, cosistent with Brooks et al. Split the 366 days into two 183-day long portions. Add bottom 183-day portion to the top of the data and the top 183-day portion to the bottom of the data. The result is a 732-day long data. Apply frequency smoothing to the middle 366 days of this 732-day data.

```{r}
doy_prob <- rbind(doy_prob[184:366, ], doy_prob, doy_prob[1:183, ])

sigma_t <- 5 # temporal smoother
time_wts5 <- exp(-0.5 * (c(-183:182)/ sigma_t)^2) / (sqrt(2 * pi) * sigma_t)
# ensure weights sum to 1
sum(time_wts5)

sigma_t <- 15 # temporal smoother
time_wts15 <- exp(-0.5 * (c(-183:182)/ sigma_t)^2) / (sqrt(2 * pi) * sigma_t)
# ensure weights sum to 1
sum(time_wts15)

doy_prob$freq_smooth5 <- 0
doy_prob$freq_smooth15 <- 0
for (eachDay in c(184:549)) {
  beg_index <- eachDay - 183
  end_index <- eachDay + 182
  doy_prob$freq_smooth5[eachDay] <- sum(doy_prob$freq[beg_index:end_index] * time_wts5) 
  doy_prob$freq_smooth15[eachDay] <- sum(doy_prob$freq[beg_index:end_index] * time_wts15) 
}

# discard half cycles at the top and bottom, added previously
doy_prob <- doy_prob[c(184:549), ]
# feb 29 span of 5 days does not apply after smoothing
doy_prob$prob_smooth5 <- doy_prob$freq_smooth5 / time_span
doy_prob$prob_smooth15 <- doy_prob$freq_smooth15 / time_span

summary(doy_prob)
```

Probability of a tornado day, similar to Figure 3 of Brooks et al.

```{r}
gfx_line <- ggplot(data = doy_prob, aes(x = as.numeric(doy), y = prob, group = 1))
gfx_line <- gfx_line + geom_point()
gfx_line <- gfx_line + geom_line(aes(y = prob_smooth5, colour = "red"))
gfx_line <- gfx_line + geom_line(aes(y = prob_smooth15, colour = "blue"))
gfx_line <- gfx_line + theme(legend.position = "none")
gfx_line <- gfx_line + xlab("Day of Year") + ylab("Probability")
png("gfx_brooks2003_fig3.png")
plot(gfx_line)
garbage <- dev.off()

```
![figure 3] [brooks3]

### Figure 4, Spatial Smoothing

Select relevant data for spatial smoothing and subsequent analysis. Add temporal smoothed info to the torn 1980-99 dataset.

```{r}
torn_brooks$DATE <- substr(torn_brooks$DATE, 6, 10) # discard year
doy_prob <- doy_prob[, c("doy", "freq", "DATE", "freq_smooth15")]
torn_brooks <- merge(torn_brooks, doy_prob, by = "DATE", all = TRUE, sort = FALSE)
torn_brooks <- torn_brooks[, c("DATE", "SLAT", "SLON", "ELAT", "ELON", "LENGTH", 
                           "id", "doy", "freq", "freq_smooth15")]
# convert doy from char to int
torn_brooks$doy <- as.numeric(torn_brooks$doy)

str(torn_brooks)
```

Create a uniform grid of 80-km spanning the mainland US, consistent with Brooks et al. This 80-km grid will be in azimuthal equidistant projection. Assume 40 lat and -95 lon to be the reference for this new projection.

```{r}
# lat-lon bounds of the lower 48 states
lat_seq <- c(20, 50)
lon_seq <- c(-125, -65)
ll_df <- expand.grid(lat = lat_seq, lon = lon_seq, KEEP.OUT.ATTRS = TRUE)

# lat-lon and azimuthal equidistant projection info
ll_proj <- "+proj=longlat +datum=WGS84"
ae_proj <- "+proj=aeqd +lat_0=35 +lon_0=-95 +units=m"
```

Function to project from geographic to aeqd. Input is a data frame and the name of the columns associated with lon and lat, and the input and output projection info for CRS.

```{r}
Fn_Get_Projected_Locs <- function(in_df, lon_col, lat_col, in_proj, out_proj) {
  # create spatial data frame using sp library
  out_locs <- SpatialPointsDataFrame(coords = in_df[, c(lon_col, lat_col)], 
                                     data = in_df, 
                                     proj = CRS(in_proj))

  # project lat-lons to aeqd, using rgdal's sptransform
  out_locs <- spTransform(out_locs, CRS(out_proj))
  
  return (out_locs)
}
```

Use above to identify the bounds of the 80-km grid and the coordinates.

```{r}
ae_locs <- Fn_Get_Projected_Locs(ll_df, "lon", "lat", ll_proj, ae_proj)

# set the 80-km grid resolution and dimensions in aeqd
aegrid_res <- 80000 # raster resolution in meters 

aegrid_bounds <- apply(ae_locs@coords, 2, range)
aegrid_bounds

aegrid_xcoords <- seq(aegrid_bounds[1, "lon"], aegrid_bounds[2, "lon"], aegrid_res)
aegrid_ycoords <- seq(aegrid_bounds[1, "lat"], aegrid_bounds[2, "lat"], aegrid_res)

aegrid_xcoords
aegrid_ycoords

aeX <- length(aegrid_xcoords)
aeY <- length(aegrid_ycoords)
```

For each day identify the grids in the 80-km raster which experienced tornadoes. First, convert beginning and ending lat-lon values for each tornado segment to the equidistant projection. For some tornadoes both the beginning and ending lat-lon values are available. Interpolate between the beginning and ending coordinates and identify all the grids corresponding to these interpolated points.

Function below takes coordinates of two points (a "row" of the tornado data) and returns the 80-km grids associated with these two points and also those points lying on a straight line between them. The function returns a vector of strings, of the form "aa _ bb", where "aa" is the index in "x breaks" and "bb" is the index in "y breaks".

```{r}
Fn_Identify_Grids <- function(torn_data) {
  begLat <- torn_data["SLAT"]
  begLon <- torn_data["SLON"]
  endLat <- torn_data["ELAT"]
  endLon <- torn_data["ELON"]
  
  torn_data <- data.frame(SLAT = begLat, SLON = begLon, ELAT = endLat, ELON = endLon)
  
  if (!(begLat == 0 | begLon == 0 | sum(any(is.na(torn_data))) > 0)) {
  
    out_list <- list()
  
    # indices of beginning lat-lon, corresp to "SLON" & "SLAT"
    sloc <- Fn_Get_Projected_Locs(torn_data, "SLON", "SLAT", ll_proj, ae_proj)
    sloc <- sloc@coords
    
    out_list <- c(out_list, 
                  paste0(findInterval(sloc[2], aegrid_ycoords), "_", 
                         findInterval(sloc[1], aegrid_xcoords)))
    
    if (endLat != 0 & endLon != 0) {
      if (abs(begLat - endLat) > 0.0 | abs(begLon - endLon) > 0.0) {
        # indices of ending lat-lon, corresp to "ELON" & "ELAT"
        eloc <- Fn_Get_Projected_Locs(torn_data, "ELON", "ELAT", ll_proj, ae_proj)
        eloc <- eloc@coords
        
        out_list <- c(out_list, 
                    paste0(findInterval(eloc[2], aegrid_ycoords), "_", 
                           findInterval(eloc[1], aegrid_xcoords)))
  
        # indices of in-between points
        # interpolate by making cuts at 5-km intervals
        if (eloc[2] > sloc[2]) {
          bet_y <- seq(sloc[2], eloc[2], 5000)
        } else {
          bet_y <- seq(sloc[2], eloc[2], -5000)
        }
        
        if (eloc[1] > sloc[1]) {
          bet_x <- seq(sloc[1], eloc[1], 5000) 
        } else {
          bet_x <- seq(sloc[1], eloc[1], -5000)
        }
        bet_y <- findInterval(bet_y, aegrid_ycoords)
        bet_x <- findInterval(bet_x, aegrid_xcoords)
        bet_indices <- paste0(bet_y, "_", bet_x)
        out_list <- c(out_list, bet_indices)
      } 
    }
    
    # identify unique indices
    out_list <- unlist(out_list, use.names = FALSE)
    out_list <- unique(out_list)
    
    return (out_list)
  } else {
    return (NA)
  }
}
```

Use the above function to identify the grids with tornado occurrences for each day of the year. Assign the temporally smoothed frequency, for that day, to each grid.

```{r}
space_raw <- array(data = 0, dim = c(aeY, aeX, 366))
for (eachDoy in c(1:366)) {
  # data for each doy
  torn_doy <- subset(torn_brooks, doy == eachDoy)
  torn_doy <- as.matrix(torn_doy[, c("SLAT", "SLON", "ELAT", "ELON", "freq_smooth15")])
  
  # grids corresp to all lat-lon pairs for the doy
  doy_grids <- apply(torn_doy, 1, FUN = Fn_Identify_Grids)
  if (sum(any(is.na(doy_grids))) == 0) {
    doy_grids <- unique(unlist(doy_grids, use.names = FALSE))
    
    # convert these indices to locations in the lat-lon matrix
    ll_index <- do.call("rbind", strsplit(doy_grids, "_"))
    ll_index <- as.matrix(ll_index)
    ll_index <- apply(ll_index, c(1, 2), FUN = as.numeric)
    
    # sometimes, the y-index ("lat") is 0 - because aeqd system is either quirky 
    # or the transformation is not properly done or i am doing something wrong
    # for now, assign 1 to these 0 values
    ll_index[, 1] <- ifelse(ll_index[, 1] < 1, 1, ll_index[, 1])
    
    # populate the parent array
    for (eachRow in c(1:nrow(ll_index))) {
      space_raw[ll_index[eachRow, 1], ll_index[eachRow, 2], eachDoy] <- unique(torn_doy[, "freq_smooth15"])
    }
  }
}
```

Calculate spatial weights, using Equation 2. This equation in the paper is incomplete! - the gaussian 2-D KDE integral was represented as a double integral, but the delta x and delta y terms for the I and J directions have been left out. Also in equation 2 of the paper, the left hand side should be f subscript x,y,n and not probability p. 

Check the KDE weights for select grids in the middle of the grid and near or along the edges and ensure they add up to 1.

```{r}
# Function to compute the euclidean distance between 2 points
Fn_Compute_Distance <- function(y1, x1, y2, x2) {
  return (sqrt((y1 - y2)^2 + (x1 - x2)^2))
}

# matrices used in distance calcs
xindx_mat <- matrix(rep(c(1:aeX), aeY), nrow = aeY, byrow = TRUE)
yindx_mat <- matrix(rep(c(1:aeY), aeX), nrow = aeY, byrow = FALSE)

aegrid_res_km <- aegrid_res / 1000 # grid resolution in km
sigma_x <- 120 # spatial smoother in km

for (eachRow in c(1, 2, 5, 15, aeY)) {
  for (eachCol in c(1, 2, 5, 38, aeX)) {
    # calculate distance matrix
    dist_mat <- aegrid_res_km * Fn_Compute_Distance(yindx_mat, xindx_mat, 
                                                          eachRow, eachCol)
    # calculate sum of weights
    space_wts <- exp(-0.5 * (dist_mat / sigma_x)^2) * (aegrid_res_km ^ 2) / 
      (2 * pi * (sigma_x ^ 2))                         
    
    cat("Y = ", eachRow, ", X = ", eachCol, ", sum = ", round(sum(space_wts), 2), "\n")
  }
}
```

The weights add up to 1 for most of the grids, except the ones on or very close to the edge of the grids. For these grids, scale the weights such that their sum is 1.

```{r}
space_smooth <- array(data = 0, dim = c(aeY, aeX, 366))

for (eachRow in 1:aeY) {
  for (eachCol in 1:aeX) {
    # calculate distance matrix
    dist_mat <- aegrid_res_km * Fn_Compute_Distance(yindx_mat, xindx_mat, 
                                                          eachRow, eachCol) 
    # spatial smoothing, equation 2                   
    for (eachDoy in c(1:366)) {
      # calculate weights
      space_wts <- exp(-0.5 * (dist_mat / sigma_x)^2) * (aegrid_res_km ^ 2) / 
        (2 * pi * (sigma_x ^ 2))
      # ensure weights add up to 1
      space_wts <- space_wts / sum(space_wts)

      # smooth frequencies
      space_smooth[eachRow, eachCol, eachDoy] <- sum(space_raw[, , eachDoy] * space_wts)                                         
    }
  }
}

```

Some diagnostic plots on smoothed spatial frequency.

```{r}
# map of the lower 48 in aeqd
usa_map <- map("state", xlim = range(lon_seq), ylim = range(lat_seq), plot = FALSE)
usa_map <- map2SpatialLines(usa_map)
proj4string(usa_map) <- CRS(ll_proj)
usa_map <- spTransform(usa_map, CRS(ae_proj))

Fn_Draw_Spatial_Freq <- function(doy, gfx_name, smoothed = TRUE, cumulative = FALSE) {
  if (cumulative) {
    doy_rast <- apply(space_smooth[,,c(1:doy)], c(1,2), FUN = sum)
    if (!smoothed) {
      doy_rast <- apply(space_raw[,,c(1:doy)], c(1,2), FUN = sum)
    }
  } else {
    doy_rast <- space_smooth[,,doy]
    if (!smoothed) {
      doy_rast <- space_raw[,,doy]
    }
  }
  
  # flip the matrix from S-N to N-S to counteract "raster" package behavior
  doy_rast <- doy_rast[c(nrow(doy_rast):1), ]
  # plot tornado days per year for cumulative
  if (cumulative) {
    doy_rast <- doy_rast / 366
    # if tornado days are less than 0.25, set to NA
    doy_rast[doy_rast < 0.25] <- NA
  }
  
  doy_rast <- raster(doy_rast, 
                     xmn = min(aegrid_xcoords), 
                     xmx = max(aegrid_xcoords), 
                     ymn = min(aegrid_ycoords),
                     ymx = max(aegrid_ycoords), 
                     crs = ae_proj) 
  
  png(paste0(gfx_name, ".png"), width = ncol(doy_rast)*10, height = nrow(doy_rast)*10)
  if (cumulative) {
    leg_breaks <- c(0.25, seq(0.5, 3, 0.5))
    plot(doy_rast, 
         breaks = leg_breaks, 
         col = rev(rainbow(12)), 
         lab.breaks = leg_breaks, 
         zlim = range(leg_breaks), 
         axes = FALSE, 
         main = paste0("Number of Tornadoes Per Year, ", beg_year, "-", end_year))
  } else {
    plot(doy_rast, axes = FALSE)
  }
  plot(usa_map, add = TRUE)
  if (smoothed) {
    if (cumulative) {
      contour(doy_rast, levels = leg_breaks, add = TRUE, axes = FALSE)
    } else {
      contour(doy_rast, add = TRUE, axes = FALSE)
    }
  }
  garbage <- dev.off()  
}

# tornado days per year
Fn_Draw_Spatial_Freq(366, "gfx_brooks2003_fig4", smoothed = TRUE, cumulative = TRUE)

# # jun 1
# Fn_Draw_Spatial_Freq(153, "doy_indiv_smooth", smoothed = TRUE, cumulative = FALSE)
# Fn_Draw_Spatial_Freq(153, "doy_indiv_raw", smoothed = FALSE, cumulative = FALSE)

```
![figure 4] [brooks4]

[brooks1]: gfx_brooks2003_fig1.png "figure 1"
[brooks2]: gfx_brooks2003_fig2.png "figure 2"
[brooks3]: gfx_brooks2003_fig3.png "figure 3"
[brooks4]: gfx_brooks2003_fig4.png "figure 4"
